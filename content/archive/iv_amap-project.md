+++
title = "项目总结"
date = 2025-05-06T12:38:52+08:00
draft = false
description = "会员项目相关总结"
subtitle = "Raḥimahu Allāh"
header_img = ""
short = false
toc = true
tags = []
categories = ["Computer Science"]
series = ["工作项目"]
comment = false
summary = ""
hidden = true
+++

# 甲: 积分兑换系统全景总结（含同步与异步版本对比、设计哲学）
> 里程兑换又上新了。真棒。又是怀念我J总、HB老板和H哥的一天。

### 系统关键目标

* 高并发下安全的积分扣减和商品发放
* 明确交易状态，确保最终一致性
* 兼顾用户体验与系统可维护性

## 核心流程图（异步落账版本，商品同步）

```text
[用户请求]
    ↓
[前置校验]（风控、兑换资格、次数）
    ↓
[积分预扣阶段（事务）]
    - 查询总表判断积分是否足够
    - Redis DECR 次数、疲劳度
    - 乐观锁扣减积分表 + 插入 right_record = prelock
        - UPDATE user_points WHERE points = 原值
        - INSERT right_record
    ↓
[调用商品服务（同步）]
    ↓
  ┌────────────┬──────────────┐
  │            │              │
成功         失败（或超时）   异常场景
  │            │              │
  ↓            ↓              ↓
[更新 right_record = pending]   [更新为 cancel + 回滚 Redis + 补偿积分表]
[发布积分核销事件]
  ↓
[异步 Worker 消费事件]
  ↓
[积分明细落账处理（事务）]
    - 删除 & 拆分明细
    - 更新 right_record = success
```

## 核心状态机（right\_record.status）

| 状态      | 含义           |
| ------- | ------------ |
| prelock | 积分预扣成功，未发货   |
| pending | 商品发货成功，积分待落账 |
| success | 核销完成，交易完成    |
| cancel  | 任意失败，整体回滚    |

## 🔍 核心设计点说明

### 1. 乐观锁的使用（积分总表）

* 查询积分后在更新时加 `WHERE points = 原值`
* 防止并发下出现双重扣减或超发
* 未命中时直接视为失败，避免锁竞争

### 2. 商品服务为什么保留同步？

* 商品发放是交易中**最关键的操作**：一旦发货不能撤回
* 异步发货会带来以下风险：

  * 无法准确确认是否发出
  * 回查接口不一定可靠
  * 需要复杂补偿逻辑
* 保留同步调用可以：

  * 明确交易分界点
  * 避免二次发货
  * 提高可控性

> Trade-off：牺牲部分吞吐换来交易确定性

###  3. 异步化为什么只做积分落账？

* 原因：明细落账是数据库中**最耗时**的操作

  * 查询 + 多条删除 + 拆分更新
* 用户感知度低（核销可晚点完成）
* 商品已发货不可撤销，因此积分落账放后异步处理

### 4. right\_record 的角色

* 交易锚点：状态全由它驱动
* 幂等保证：无论同步/异步失败后重试都可以靠它定位
* 监控与补偿：系统只需扫描 prelock / pending 记录即可

## 各阶段事务说明

| 阶段     | 是否事务 | 内容                                  |
| ------ | ---- | ----------------------------------- |
| 积分预扣阶段 | ✅    | Redis 扣次数（非事务） + MySQL 扣积分、插入记录（事务） |
| 商品服务调用 | ❌    | 同步调用，决定是否进入异步落账                     |
| 积分明细落账 | ✅    | 删除、拆分明细，更新状态                        |
| 回滚流程   | 部分事务 | 积分表回加（事务），Redis 回滚（非事务）             |

## 补偿机制 & 幂等处理

* 补偿扫描：right\_record where status in ('prelock', 'pending')
* 异步落账幂等处理：检查 right\_record.status 是否已为 success
* Redis 回滚失败记录：补偿任务执行 Lua 脚本自动恢复
* 商品服务失败：走 retry + cancel 路径

## Trade-off 一览

| 决策       | 价值            | 代价                  |
| -------- | ------------- | ------------------- |
| 商品服务同步处理 | 明确交易边界，确保发货安全 | 牺牲部分吞吐              |
| 落账异步处理   | 减少用户等待，提升 RT  | 增加补偿和监控复杂度          |
| 乐观锁扣积分   | 无需锁表，抗并发强     | 需配合幂等防止重复扣减         |
| 不冻结明细    | 实现简单          | 需 fallback 容忍部分过期冲突 |

## 推荐优化路径（未来演进）

* 使用 Kafka / NSQ 替代内建 eventBus，实现跨服务可靠消费
* right\_record 支持多子状态：落账中、补偿中等
* 明细层面增加“冻结”状态，防止核销中被后台清除
* 积分总表刷新的 worker 做 checksum 与审计校验
* 引入 trace ID 全链路追踪

## 总结：这是一套精心平衡“吞吐、准确性、可控性”的积分系统架构

* 发货必须成功 → 同步，确认边界
* 落账可以稍后 → 异步，提升性能
* 异步机制 + 状态机 + 幂等机制确保最终一致
* 整体设计允许局部故障，但能最终收敛、可补偿


# 乙: 弹幕系统设计方案（高并发、低延迟、4个9高可用）
> 这面试官长得可真像QHY，怀念我曾经好兄弟的一天。

## 一、目标与挑战

- 支持百万级并发连接，低延迟实时消息传输
- 架构具备高可用、易扩展、易维护
- 核心关注推送链路（接收弹幕）性能与稳定性

## 二、整体架构简述（逻辑层次）

1. **客户端 WebSocket 长连接接入**
2. **WebSocket 网关负责连接池管理和消息推送**
3. **发弹幕走普通业务链路（HTTP / RPC）**
4. **业务服务处理逻辑、异步持久化、消息入队**
5. **消息队列（MQ）做弹幕广播中转**
6. **推送服务消费 MQ 消息，按房间广播推送**

## 三、链路设计

### 1. 发弹幕链路（客户端 → 服务端）
- 客户端发弹幕使用 HTTP / RPC 请求
- 业务服务校验 + 异步写库 + 投递到 MQ

### 2. 弹幕广播链路（服务端 → 客户端）
- MQ 作为消息中心（Kafka / 内存 MQ）
- 推送服务消费后 fanout 到房间内用户
- 网关根据房间连接列表，将消息通过 WebSocket 推送

## 四、关键模块设计

### WebSocket 网关
- 长连接维护，支持水平扩展（如 Redis + 一致性哈希）

### 消息队列（MQ）
- 每个房间一个逻辑 Topic，支持批量、压缩、高吞吐

### 推送服务
- 消费 MQ 消息，批量广播到活跃连接

## 五、性能与可用性优化

| 优化点 | 策略 |
|--------|------|
| 发弹幕延迟 | 异步落库 |
| 广播效率 | 批量推送、合并帧 |
| 高可用性 | 多实例部署，跨 AZ 容灾 |

## 六、设计取舍说明：发弹幕为何不走 WebSocket？

- 单用户发弹幕为低频操作，适合走 HTTP 请求
- WebSocket 专注于广播推送链路，职责更清晰
- 便于限流、鉴权、打点等通用请求控制策略

## 七、精华答题版（2 分钟表达）

我会把弹幕系统分为两个核心链路：**发弹幕**和**推弹幕**。

- 发弹幕我采用常规的 **HTTP/RPC 链路**，因为用户发弹幕是低频行为，不需要长连接，走短链路更适合做限流、鉴权、埋点，也易于维护。
  
- 推弹幕走 **WebSocket + 消息队列（MQ）链路**。客户端与网关维持长连接，服务端通过业务逻辑将弹幕写入 MQ，推送服务按房间消费后，fanout 给所有在线用户。这样能实现毫秒级推送。

- 架构上我做了解耦：客户端连网关，业务服务异步处理并落库，推送层独立消费推送。WebSocket 网关支持水平扩展，通过连接状态表或 Redis 路由。

- 为了高可用，所有服务都可无状态部署，多实例部署在多机房，结合心跳、重连机制和状态热备保证整体 SLA。

这种设计能保证大规模并发下的 **低延迟、稳定性与弹性扩展性**，同时逻辑清晰，职责划分明确，适合生产环境落地。


# 	丙: 秒杀系统设计 - 同步与异步双版本对比总结
> 面向高并发场景下的限时抢购/抽奖系统，强调稳定性、并发控制、正确性与用户体验。

* 支撑高并发瞬时请求（10w \~ 1000w QPS）
* 避免超卖 / 重复抢购
* 快速反馈、良好用户体验
* 系统可扩展、可降级、可回放

## 🚀 同步秒杀系统设计（轻量高效型）

### 关键特征：

* 用户请求同步返回“是否抢购成功”
* 所有核心逻辑在内存/缓存内完成，响应超快
* 适合中小型促销、低延迟要求

### 核心机制：

| 步骤              | 说明                                     |
| --------------- | -------------------------------------- |
| 1. 鉴权 + 限流      | 用户身份校验、IP限速、QPS控制（网关层 + Nginx + Redis） |
| 2. 秒杀时间判断       | 服务层快速校验是否在允许时间窗口内                      |
| 3. Redis 原子校验脚本 | Lua 脚本判断库存、用户是否参与，扣减库存并记录成功            |
| 4. 返回结果         | 立即告诉用户抢购成功/失败/重复                       |
| 5. 异步补偿（可选）     | 成功用户写入异步队列做落单、通知、物流等操作                 |

## 🚀 大促型异步秒杀系统设计（弹性高可用型）

### 关键特征：

* 请求快速入队，用户收到“排队中”/“提交成功”响应
* 处理链路异步化，松耦合、支持削峰填谷、系统隔离
* 用户状态查询/推送分离于主链路

### 典型处理链路

```text
[用户请求]
    ↓
[网关/接入层] —— 限流 + 灰度 + token校验
    ↓
[预处理服务]
    ├─ 检查用户参与资格 + 幂等
    ├─ Redis 判断库存是否可扣（可选预扣）
    └─ 入队 Kafka（order.create.topic）
    ↓
返回：排队中 / 接收成功

[消息队列消费者]
    ↓
Worker Pool 批处理消息
    ↓
数据库写入订单、扣库存、发券等操作
    ↓
写入结果状态缓存：Redis `ms:result:<uid>:<sku>`

[用户查询接口 / WebSocket 通知]：展示是否成功
```
### 设计关键点拆解

#### 入队前防刷 + 幂等

* 网关限流、IP滑动窗口、验证码校验
* Redis + SETNX 防重复提交
* 每次请求带 token，保证唯一性

#### 请求排队削峰

* 使用 Kafka/RabbitMQ 分区/顺序队列做入队
* 写入失败时快速返回错误码，不处理

#### Worker 消费处理

* 支持批量拉取/写库（节省 DB 压力）
* 加强幂等处理（如事务外幂等记录表）
* 落库失败：写入失败队列，后续重试

#### 用户态异步反馈

* Redis 存储处理状态（成功/失败/处理中）
* 客户端定时轮询 or WebSocket 收到通知

#### 可回放/补偿机制

* Kafka 持久化消费位点，支持失败回放
* 支持人工重试机制，后台运维介入手段

#### 流量打散

* 抢购开始前 token 预热，接口灰度开放
* 秒杀时间多批次分片，用户随机分组进入

#### 降级策略

* Redis 查询失败 → 返回“排队中”，不崩溃
* Kafka 写入失败 → 限制流量 / fallback 至 Redis List

### 技术组件选型建议

| 环节   | 技术建议                                  |
| ---- | ------------------------------------- |
| 消息队列 | Kafka（高吞吐） / Redis Stream（轻量）         |
| 数据层  | MySQL + 分库分表 / TiDB / OceanBase 等弹性架构 |
| 缓存   | Redis 集群，设置合理 TTL + retry 控制          |
| 服务语言 | Go / Java（高并发处理 + MQ 消费框架成熟）          |

## 核心关键设计节点（面试重点）

| 关键点        | 同步型               | 异步型（大促）                 |
| ---------- | ----------------- | ----------------------- |
| 用户是否即时知道结果 | ✅ 是               | ❌ 否（排队中）                |
| 请求入口限流     | ✅ 必须              | ✅ 必须                    |
| 是否用消息队列    | ❌ 否               | ✅ 是                     |
| Redis 作用   | 原子库存扣减 + 幂等判断     | 同左 + 入队前判断              |
| 幂等控制       | Redis SETNX 或 Lua | 同左 + MQ 消费幂等处理          |
| 如何落单       | 同步写库 或 异步任务       | 后台消费队列落库                |
| 如何通知用户     | HTTP 同步返回         | WebSocket / 轮询接口        |
| 最大并发能力     | 中高（\~10万QPS）      | 极高（\~百万QPS+）            |
| 错误处理       | 失败即返回             | 延迟补偿 / 重试队列 / offset 回放 |

## 总结（面试+实战双向准备）

* 小型活动/中小企业：**同步秒杀架构 + Redis 即可**，实现成本低，用户体验好。
* 大促活动/高并发平台：必须使用 **异步削峰 + 队列 + 状态通知 + 幂等机制**，配合限流/降级保护。
* **两种模式可以结合**：同步层只做拦截 & 入队，关键写操作交给异步后台处理。

## 幂等 Token 分发策略（可选增强机制）

### 用法概述

* 每次打开秒杀页面时后端生成 UUID token，写入 Redis 并返回前端
* 请求必须带 token 提交，后台验证 + 原子消耗 token
* Redis 存储：`SET token value EX 60 NX`

### 优势

* 统一用户行为身份标识（防止伪造/重放）
* 可前端结合按钮禁用、刷新重载校验、token 过期提示
* 可观察、可追踪、可设置 TTL

### 服务端处理逻辑

```go
val := redis.Get("ms:token:<sku_id>:<uid>")
if val == "" {
    return "非法请求（未申请或已消费）"
}
if val != clientProvidedToken {
    return "伪造请求"
}
redis.Del("ms:token:<sku_id>:<uid>") // 消费 token
```

### 与 Redis 锁的对比权衡

| 对比项           | Redis 锁（`SETNX sku:uid`） | Token 机制          |
| ------------- | ------------------------ | ----------------- |
| 是否需要生成接口      | ❌ 否                      | ✅ 需要 `/get_token` |
| 是否拦截非法请求      | ⚠️ 不行（请求已打进）             | ✅ 可提前拒绝           |
| 是否支持按钮禁用/刷新拦截 | ❌ 否                      | ✅ 是（通过 token 控制）  |
| 实现复杂度         | ✅ 简单                     | ⚠️ 略高（发、校验、消费）    |
| 防脚本攻击能力       | ⚠️ 弱                     | ✅ 强（可配合验证码）       |
| 适用场景          | 单阶段抢购请求                  | 多步骤交互、确认页、灰度控制等   |

### 综合建议

* 对于简洁秒杀系统，Redis 锁已足够防重、限量 ✅
* 若系统需支持更复杂交互（支付确认、预下单、抽奖流程等）或需要精细化防刷管控，则可引入 token ✅
* 两者可结合使用，token 控制访问门槛，Redis 锁处理最终幂等逻辑

## 异常补偿机制

* 消息消费失败（如 DB 超时）→ 推送到重试队列（或 DLQ 死信队列）
* 数据落库成功但缓存写失败 → 依赖定时任务补偿修复状态
* 订单状态一致性问题 → 定期任务核对 Redis + DB 数据是否一致
* 运维侧提供手动补单、回查工具（如基于 Kafka offset replay）


# 丁：架构设计与高可用稳定性保障总结

> 面向资深后端工程师面试场景，全面归纳项目架构、核心中间件高可用策略、稳定性保障机制与面试话术演练模版。

---

## 一、项目整体架构背景

* **所属架构**：阿里集团微服务架构整体下的单体服务项目（会员系统）
* **部署拓扑**：

  * 阿里集团 DNS → 高德网关 → 内部 VIPServer → 双机房共 40 台容器服务器 + 2 台灰度机器
  * 接入集团中间件服务：定制 Redis 分片集群、MySQL 分库分表、MetaQ 消息队列
* **业务特性**：高并发读写、用户体量大、对 SLA 要求高

---

## 二、核心组件高可用策略

### ✅ 架构分层与高可用机制

| 层级       | 架构组件 / 策略                      | 高可用说明                                    |
| -------- | ------------------------------ | ---------------------------------------- |
| 网关层      | 阿里集团 DNS、高德网关、VIPServer        | 多层负载均衡；限流、验签、请求签名校验，防止非法请求与刷流量攻击         |
| 服务层      | 双机房部署、容器化管理、灰度节点               | 灰度发布控制，服务注册发现，实例冗余，故障快速下线                |
| Redis 缓存 | 定制分片集群（16 node）主从部署            | 中间层统一路由 + 主从自动切换；多线程并发访问保障性能             |
| MySQL 数据 | 4 实例（8 库），每库 16 表，共 128 表，主从结构 | 分库分表应对高并发；一主一从容灾切换，主从同步监控，支持强一致与最终一致混合方案 |
| 消息队列     | MetaQ（集团中间件）                   | 支持消息重试、堆积监控、消费者分组隔离；底层支持高可用和事务消息保障       |

---

## 三、稳定性保障体系设计（全面展开）

### ✅ 一、中间件上云监控项覆盖

#### Redis

* 内存占用（可配置 maxmemory）
* CPU 占用
* 网络连接数、慢查询比例（monitor + slowlog）
* 主从状态（主从延迟、切主事件）
* 内部命令频率分布监控（keys/hash/zset 等）

#### MySQL

* 慢查询日志（慢 SQL 告警）
* 主从同步延迟（Seconds\_behind\_master）
* CPU / 内存占用
* 磁盘 IOPS 和 TPS/QPS 变化趋势
* 锁等待（lock wait）与连接数打满

#### 消息队列 MetaQ

* 堆积数（积压超限）
* 消费失败率
* 消费延迟时间
* 消息大小、队列分布
* HA 事件（broker 失效、leader 切换）

#### 告警通道配置

* 通知渠道：钉钉群机器人 @人、短信、语音电话
* 升级策略：3分钟内无人接手自动拨打 Leader 电话

---

### ✅ 二、服务自身监控与动态感知

#### 基础资源维度

* CPU、内存、负载（load）、网络（in/out）流量
* 容器级别的资源限制与实际使用监控（cgroup 指标）

#### 应用运行维度

* QPS、RT（平均值+TP90/TP99）、成功率、异常率（4xx/5xx）
* Error log 日志突增突降（陡增阈值、陡降可能是数据打点失败）
* 分分钟 / 日 / 周同比、环比趋势突变监控

#### 业务维度打点监控

* 核心链路行为（注册、下单、支付、登录）
* 审计操作、补偿任务执行率、异常重试率
* 特殊周期（大促、月末结算等）期间重点链路设定阈值

#### 控制策略

* Diamond 平台支持配置中心统一降级开关（动态变更）
* SOP/CDR 平台支持一键执行预案（如禁止登录、关闭某支付渠道）
* 服务层具备自动熔断机制（如 Redis 超时降级本地默认值）

---

## 四、应急处理机制总结

| 类型       | 手段                                                            |
| -------- | ------------------------------------------------------------- |
| Redis 故障 | 主从切换，客户端路由自动感知；缓存兜底 + 降级默认值策略                                 |
| MySQL 异常 | 支持从库查询 fallback，主从切换后保障写请求，慢 SQL 热点分析优化                       |
| MQ 消费堆积  | 提升消费并发数，降级非关键异步流程，补偿队列兜底                                      |
| 容器异常     | 容器探针失效自动重启，下线故障实例，统一日志拉取分析                                    |
| 灰度失败     | 灰度机器独立隔离，发布失败可快速回滚；发布监控绑定业务埋点自动报警                             |
| 热点打满     | Redis 热 key 打散（key hash 前缀+hash tag），DB 热点表拆分                 |
| 日志爆量     | 限制日志打印频率、降采样、分 log 级别流控                                       |
| 网关雪崩     | QPS 超限统一限流策略（token bucket / leaky bucket），配置 fallback handler |

---

## 五、常见面试追问与行业通用方案汇总

| 问题                | 面试答法与实战经验结合                                                                                      |
| ----------------- | ------------------------------------------------------------------------------------------------ |
| Redis 主挂后多久恢复？    | 依赖中间件层的故障检测（默认 5s 内 failover），客户端路由自动感知。我们后续通过提升检测频率和 metrics 精度，将整体 failover 延迟控制在 2s 内。        |
| 如何控制 Redis 热 key？ | 实际场景中我们遇到过评论类 ID 集中写入，使用 hash tag + 多份冗余 key 分散热点；并结合本地 LRU 缓存降频访问频率。                            |
| MQ 堆积如何缓解？        | 消费侧可水平扩容 consumer，增加 goroutine；如果任务是异步非关键型业务，可采用 delay 处理机制 + fallback 通知策略；堆积期间实时报警通过短信+钉钉快速感知。 |
| 降级场景有哪些？          | 登录异常走默认 profile、支付服务依赖熔断降级为“支付失败提示”、风控慢响应默认打 tag 等；通过 Diamond 控制开关，确保随时可启用/关闭。                   |
| 如何发现系统稳定性趋势恶化？    | 我们主要靠分钟、小时级 QPS、RT、成功率、日志量多维指标趋势监控结合同比/环比策略，能快速发现 RT 抖动、异常峰值；结合埋点定位到具体链路。                        |
| 灰度发布失败怎么办？        | 灰度节点独立配置灰度流量，出错时只影响 5% 流量用户；通过发布指标绑定打点/RT 监控，发布失败时自动回滚、报警并暂停发布。                                  |
| 降级与熔断如何配合？        | 降级通常为策略控制型（通过配置中心降功能），熔断是运行时判断（基于 RT/异常率触发自动隔离）；我们两者结合使用并自动恢复机制保障核心链路自愈。                         |

---

## 六、总结要点与准备建议

* **答题思路统一结构**：

  * 项目背景 → 架构设计 → 分层高可用策略 → 监控与应急机制 → 故障案例经验

* **关键词快速过目**：限流、灰度、failover、fallback、熔断、埋点、SOP/CDR、动态开关、热 key 打散、日志突增、自动升级告警

* **准备方式建议**：

  * 每一类组件准备 1-2 个真实演练/故障经验
  * 用四段式表达法（背景-问题-处理-反思）训练
  * 熟悉每种监控项对应指标、报警方式与排查流程

---

> ✅ 如需继续补充演练题、问题集或制作面试答辩文稿，可以继续添加模块化内容


# 戊

| **类别**   | **Hadoop/开源生态组件**                    | **阿里云/ODPS 平台对应组件**             | **说明 / 差异重点**                 |
| -------- | ------------------------------------ | ------------------------------- | ----------------------------- |
| 分布式存储    | HDFS（Hadoop Distributed File System） | MaxCompute 表底层存储 / OSS          | ODPS 底层采用专有高性能列存引擎，用户不直接操作路径  |
| 离线计算     | MapReduce / Hive SQL                 | MaxCompute SQL / PyODPS         | ODPS 用优化后的执行引擎替代 MR，性能远超 Hive |
| 资源调度     | YARN（资源分配）                           | 阿里云 MaxCompute Scheduler        | 用户无需管理资源，ODPS 是 Serverless 模型 |
| 元数据管理    | Hive Metastore                       | ODPS Project/Table/Partition 管理 | ODPS 提供统一的项目级元数据权限控制          |
| 流式计算     | Flink / Spark Streaming              | DataWorks 实时计算/Flink on EMR     | Flink 可以部署在阿里云上，与 ODPS 联动     |
| 脚本调度     | Azkaban / Oozie / Airflow            | DataWorks（调度编排平台）               | 图形化任务依赖管理，支持 SQL/Python 等混合任务 |
| 日志采集     | Flume / Kafka / Logstash             | SLS（日志服务）/ TT（日志总线）             | 更适合业务埋点、日志实时采集到数据平台           |
| NoSQL 存储 | HBase（KV 数据库）                        | HBase on 云原生 EMR / Tablestore   | 需要低延迟读写时选用；ODPS 不适合频繁写        |
| 可视化 BI   | Superset / Tableau / Redash          | FBI（QuickBI / 自助取数）             | BI 展示和运营自助查询分析平台              |

| **你常做的事**              | **标准术语（大数据团队）**                                  |
| ---------------------- | ------------------------------------------------ |
| 埋点、日志打 TT / Kafka      | 数据采集（Data Ingestion）                             |
| Flink 实时清洗打点数据         | 实时 ETL / 流式 ETL（Streaming ETL）                   |
| MySQL 数据 T+1 导入 ODPS   | 离线采集 / 离线同步（Batch Ingestion）                     |
| SQL 聚合、统计、Join 运算      | 离线计算 / 指标建模（Batch Computation / Metric Modeling） |
| pyodps 调 SQL、脚本定时跑     | 任务调度（Scheduler，例如调度系统为 Zeus、Airflow 等）           |
| Flink 计算写 TT 表或落回 ODPS | 实时入库（Real-time Sink）                             |
| ODPS 表结果供 FBI 平台查看     | 自助 BI / 可视化查询平台（BI平台，如 FBI、QuickBI）              |
| FBI 展示指标、用户留存、趋势等      | 数据服务（Data Service）                               |